---
title: "Statistical Modelling of bird and cetacean distributions in offshore renewables development areas"
author: "LAS Scott-Hayward, C Oedekoven, CG Walker and ML Mackenzie"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: newref.bib
---


-------

#### This vignette constitutes work carried out at the Centre for Research into Ecological and Environmental Modelling (CREEM) at the University of St. Andrews.


**Please reference this document as:**
Scott-Hayward, L.A.S., Oedekoven, C.S., Mackenzie, M.L. and Walker, C.G. (2020). Vignette for the MRSea Package v1.02: Statistical Modelling of bird and cetacean distributions in offshore renewables development areas. Centre for Research into Ecological and Environmental Modelling, University of St Andrews.

*******

```{r echo=FALSE, message=FALSE, warning=FALSE}
require(knitcitations)
cleanbib()
biblio <- read.bibtex("newref.bib")
cite_options(citation_format = 'pandoc', cite.style = 'authoryear', max.names = 1, longnamesfirst=FALSE)
knitr::opts_chunk$set(fig=TRUE, warning=FALSE, message=FALSE, eval=TRUE, comment = '')
```

## Introduction

The ```MRSea``` package was developed for analysing data that was collected for assessing potential impacts of renewable developments on marine wildlife, although the methods are applicable to other studies as well. This vignette gives an updated example of the code for version 0.3.  For additional information regarding methods, see `r citet(biblio[['Mac2013']])` and `r citet(biblio[['ScottH2013a']])`.  The user should be familiar with generalised linear models and their assumptions and model selection. The ```MRSea``` package primarily allows spatially adaptive model selection for both one and two dimensional covariates using the functions ```runSALSA1D``` and ```runSALSA2D```, which implement the methods of `r citet(biblio[['Walker2010']])` and `r citet(biblio[['ScottH2013']])`. 

The major update to this package is that a class of model `gamMRSea` is created when running either SALSA 1D or 2D. This retains within the model object information regarding fitting, such as the `splineParam` object and the panel structure (if present).  The use of the `summary` function on these models returns both raw and robust standard errors, with the *p*-values from the models hypothesis test using the robust standard errors.  The robust standard errors are obtained using the panel structure given (independence is one panel per data point and is the default if no structure is given).

Other functions include diagnostics (to assess residual correlation: ```runACF```, smooth relationships: ``` runPartialPlots``` and model selection (ANOVA) for robust standard errors: ```anova.gamMRSea```) and inference (```do.bootstrap.cress```). 


![alt text](MRSea_workflow.png)
Example of the modelling process using MRSea.  Packages with functions to run certain parts are given in oval boxes.  To complete the modelling process, other packages may be used at certain stages.  These are coded light blue, whilst MRSea functions are in red.  GEE (far right), stands for Generalised Estimating Equations.


A full description of each of the functions within the ```MRSea``` package can be found in the reference manual at: 
http://creem2.st-and.ac.uk/software.aspx.  Note that the manual uses version 0.2.2 of MRSea.

## Distance sampling using the ```mrds``` library

1. Load data and fit detection function (Distance Sampling)
```{r message=FALSE}
#devtools::load_all(path='../../MRSea/')
require(MRSea)
# we will use the dataset with a known re-distribution of animals
data(dis.data.re)
dis.data<-dis.data.re
require(mrds) # distance sampling package
result <- ddf(dsmodel=~mcds(key="hn", formula=~1),
              data = dis.data, method="ds", 
              meta.data=list(width=250))
```

2. Adjust sightings for detectability 

```{r dist, cache=TRUE, results='hide', warning=FALSE, message=FALSE}
# create.NHAT and create.count.data are MRSea functions to adjust the 
# sightings for the detection function estimated above.
dis.data <- create.NHAT(dis.data,result)
count.data <- create.count.data(dis.data)
```

3. Try a simple model

```{r message=FALSE, warning=FALSE}
data <- count.data
data$response <- round(data$NHAT)
attach(data)
fullModel <- glm(response ~ as.factor(season) + as.factor(impact) +
                   depth + x.pos + y.pos, family = poisson, data = data)
```

4.  Try a model with a smooth term for depth

```{r message=FALSE}
require(splines)
fullModel <- glm(response ~ as.factor(season) + as.factor(impact) +
                   bs(depth, knots = mean(depth)) + x.pos + y.pos, 
                 family = poisson,data = data)
```

5.  If the data are correlated then you may wish to specify a blocking structure in your dataset. 


For correlated data:
```{r }
data$blockid <- paste(data$transect.id, data$season, data$impact,sep = "")
```


## Selection of 1D Covariates

Run SALSA1D to select what covariates are included and whether or not they are smooth.  SALSA selects the smoothness of each term (number and location of knots) and 10-fold CV is used to choose between the best smooth term, a linear term or no term at all.  To not allow the removal process the user may set `removal = FALSE` as a parameter in the function `runSALSA1D`.

6.  If you wish to make predictions once the model is fitted, then a prediction grid should be created and specified.  This is because the splines fitted here (B-splines) are unable to make predictions outside of the range they were created.  For example, if the data range for depth is smaller than the range of depths in the prediction data, predictions cannot be made.  

```{r }
data("nysted.predictdata")  # contains predict.data
# This is a spatial grid for making predictions.  All covariates in 
# final model must be in this data frame and the naming must be the 
# same as for the data
predictData <- nysted.predictdata
range(data$depth)
range(predictData$depth)
```

Here the range of the predictions is slightly wider than the range of the data, so we will specify ```predictData``` when running SALSA.

7. Set up the initial model with factor covariates and the offset term (if required) and specify the parameters required:

```{r message=FALSE}
initialModel <- glm(response ~ as.factor(season) + as.factor(impact) 
                    + offset(log(area)), family = "quasipoisson", 
                    data = data)
```

```{r }
salsa1dlist <- list(fitnessMeasure = "cv.gamMRSea", 
                    minKnots_1d = 2,
                    maxKnots_1d = 5, 
                    startKnots_1d = 1, 
                    degree = 2, 
                    maxIterations = 10,
                    gaps = c(0), 
                    cv.opts=list(cv.gamMRSea.seed=1, K=10))
```

8. Run SALSA;

```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# run SALSA
require(MuMIn)
salsa1dOutput <- runSALSA1D(initialModel, salsa1dlist, c("depth"),
                      predictionData=predictData, datain=data, removal=TRUE, 
                      panelid = data$blockid)
```

```{r eval=FALSE}
# run SALSA
salsa1dOutput <- runSALSA1D(initialModel, salsa1dlist, c("depth"),
                      predictionData=predictData, datain=data, removal=TRUE, 
                      panelid = data$blockid)
```

Use the built in summary function (`summary.gamMRSea`) to look at the summary of the model.  Note that robust standard errors are given alongside the raw standard errors and information regarding panels is at the bottom of the output. If each data point is a panel, then independence is assumed.

```{r }
summary(salsa1dOutput$bestModel)
```


```{r }
# How many knots were chosen for depth?
salsa1dOutput$splineParams[[2]]$knots
# ~~~~~~~~~~~~~~~~~~~~~~~
```


## Selection of flexibility for 2D smooth term

9. Create a grid of knots that will be used as possible knot locations.  This may take while and could be different every time you run it so I suggest saving the knotgrid as a file.

```{r knotgrid, message=FALSE, fig=TRUE, fig.align='center', fig.width=9, fig.height=6, cache=TRUE}
knotgrid<- getKnotgrid(coordData = cbind(data$x.pos, data$y.pos), numKnots = 300)
#
# write.csv(knotgrid, file='knotgrid_fullanalysis.csv', row.names=F)
# ~~~~~~~~~~~~~~~~~~~~~~~
```

The black points in the figure are the data and the red points, the candidate knot locations. By default, the knotgrid has 300 knot positions chosen.

10. Set up parameters for SALSA2D.  Distance matrices (data to knots and knot to knots), a fit statistic and min, max and start knots.

```{r }
# make distance matrices for datatoknots and knottoknots
distMats <- makeDists(cbind(data$x.pos, data$y.pos), knotgrid)

# ~~~~~~~~~~~~~~~~~~~~~~~

# make parameter set for running salsa2d
salsa2dlist<-list(fitnessMeasure = 'cv.gamMRSea', 
                  knotgrid = knotgrid,
                  startKnots=10, 
                  minKnots=4, 
                  maxKnots=15, 
                  gap=0, 
                  interactionTerm="as.factor(impact)", 
                  cv.opts=list(cv.gamMRSea.seed=1, K=10))
```

11. Run SALSA2D to find the appropriate number and location of knots for the 2D smooth term of `x.pos` and `y.pos`.

```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
salsa2dOutput<-runSALSA2D(salsa1dOutput$bestModel, 
                          salsa2dlist, 
                          d2k=distMats$dataDist,
                          k2k=distMats$knotDist)
```

```{r echo=TRUE, eval=FALSE}
salsa2dOutput<-runSALSA2D(salsa1dOutput$bestModel, 
                          salsa2dlist, 
                          d2k=distMats$dataDist, 
                          k2k=distMats$knotDist)
```

```{r }
require(ggplot2)
ggplot() + geom_point(data=data, aes(x=x.pos, y=y.pos), colour='grey') +
  theme_bw() + 
  geom_point(data=data.frame(knotgrid), aes(X1, X2), col='blue') + 
  geom_point(data=data.frame(knotgrid)[salsa2dOutput$aR[[1]],], aes(X1, X2), col='darkgreen', size=4) 

```

12.  Are the residuals correlated? Make a suitable blocking structure, within which residuals are expected to be correlated but between which they are independent.  Use `runACF` to assess the blocking structure.

```{r acfplot, fig.cap='ACF plot showing correlation in each block (grey lines), and the mean correlation by lag across blocks (red line).'}
runACF(block = data$blockid, model = salsa2dOutput$bestModel, suppress.printout=TRUE)
```

Here we also do a runs test to assess for correlation in the model residuals.  Since our data are over-dispersed, we must use the empirical distribution for assessment:

```{r}
simData<-generateNoise(n=500, response=fitted(salsa2dOutput$bestModel), family='poisson', d=summary(salsa2dOutput$bestModel)$dispersion)
empdist<-getEmpDistribution(500, simData, salsa2dOutput$bestModel, data=data,dots=FALSE)
runsTest(residuals(salsa2dOutput$bestModel, type='pearson'),emp.distribution=empdist)
```


13. Model selection
```{r }
set.seed(1)
# 2D model
cv.gamMRSea(data=data, modelobject = salsa2dOutput$bestModel, K=10)$delta[2]
# salsa2dOutput$fitStat # alternatively

# 1D model
cv.gamMRSea(data=data, modelobject = salsa1dOutput$bestModel, K=10)$delta[2]

# intitial model
cv.gamMRSea(data=data, modelobject = initialModel, K=10)$delta[2]

# full glm model with simple depth smooth
cv.gamMRSea(data=data, modelobject = fullModel, K=10)$delta[2]

```

```{r}
anova(salsa2dOutput$bestModel)
```

```{r fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
par(mfrow=c(2,2))
runPartialPlots(model = salsa2dOutput$bestModel, data = data, 
                factorlist.in = c('season', 'impact'), 
                varlist.in = 'depth', showKnots = T, 
                includeB0 = TRUE)
```

```{r fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
par(mfrow=c(2,2))
runPartialPlots(model = salsa2dOutput$bestModel, data = data, 
                factorlist = c('season', 'impact'), varlist = 'depth', 
                showKnots = T, type='link', 
                includeB0 = TRUE)
```

## Making Predictions

```{r }
preddist<-makeDists(cbind(predictData$x.pos, predictData$y.pos), 
                 knotgrid,knotmat=FALSE)$dataDist


# make predictions on response scale
preds<-predict.gamMRSea(newdata = predictData, g2k = preddist, object = salsa2dOutput$bestModel)
```

Plotting the predictions pre and post impact:

```{r fig=TRUE, fig.align='center', fig.width=9, fig.height=4}
require(RColorBrewer)
predictData$preds<-preds[,1]
ggplot() + geom_tile(data=predictData, aes(x.pos, y.pos, fill=preds), height=0.5, width=0.5) +
  facet_wrap(~impact + season, nrow=2) + theme_bw() + coord_equal() + 
  scale_fill_distiller(palette = "Spectral",name="Animal Counts")
```

## Bootstrapped Confidence Intervals and Difference Surfaces

Note that the coding in this section has changed slightly from the original user guide.

14. Bootstrap to include parameter estimation uncertainty in the detection function and parameter estimation in the spatial model. (Note: If no detection function estimated, then the bootstrap is just on the parameters of the spatial model.)

```{r boots, warning=FALSE, message=FALSE, results='hide'}
dis.data$seasonimpact <- paste(dis.data$season, dis.data$impact)

bootPreds<-do.bootstrap.cress.robust(model.obj = salsa2dOutput$bestModel, 
                                     predictionGrid = predictData,
                                     g2k=preddist,
                                     B = 100, 
                                     robust=TRUE)
```

```{r }
#load('predictionboot.RData')
cis <- makeBootCIs(bootPreds)
```

15. Calculate the differences before and after across all bootstraps
```{r }
differences <- getDifferences(beforePreds = 
                      bootPreds[predictData$impact == 0, ],
                      afterPreds = bootPreds[predictData$impact == 1, ])
```

16. Plot differences and indicate where significant positive/negative differences lie.  The grey circles indicate a significant negative difference (abundance after impact is less than the abundance before impact) and the grey crosses indicate a significant positive difference.  The colour of the cell indicates the size of the difference. 

```{r fig=TRUE, fig.align='center', fig.width=9, fig.height=6}
mediandiff <- differences$mediandiff
# The marker for each after - before difference:
# positive ('1') and negative ('-') significant differences
marker <- differences$significanceMarker
par(mfrow = c(1, 1))
quilt.plot(predictData$x.pos[predictData$impact == 0], 
           predictData$y.pos[predictData$impact == 0],
           mediandiff, asp = 1, nrow = 104, ncol = 55)
# add + or - depending on significance of cells. Just
# requires one significance out of all to be allocated
points(predictData$x.pos[predictData$impact == 0][marker == 1],
       predictData$y.pos[predictData$impact == 0][marker == 1],
       pch = "+", col = "darkgrey", cex = 0.75)
points(predictData$x.pos[predictData$impact == 0][marker == (-1)],
       predictData$y.pos[predictData$impact == 0][marker == (-1)],
       col = "darkgrey", cex = 0.75)
points(681417.3/1000, 6046910/1000, cex = 3, pch = "*", lwd = 1, col = "grey")
```

Select a single season to plot (here I have chosen season 1):

```{r fig.width=9, fig.height=6}
require(dplyr)
diffdata<-data.frame(predictData[predictData$impact==0,], mediandiff, marker)
diffdata_s1<-filter(diffdata, season==1)

wf<-data.frame(x=(681417.3/1000), y= (6046910/1000))

ggplot() + geom_tile(data=diffdata_s1, aes(x=x.pos, y=y.pos, fill=mediandiff), 
                     height=0.5, width=0.5) + 
  geom_point(data=filter(diffdata_s1, marker == 1), aes(x=x.pos, y=y.pos), 
             shape=3, colour='darkgrey', size=1) +
  geom_point(data=filter(diffdata_s1, marker == -1), aes(x=x.pos, y=y.pos), 
             shape=1, colour='darkgrey', size=1.5) +
  theme_bw() + coord_equal() +  
  scale_fill_distiller(palette = "Spectral",name="Difference") + 
  geom_point(data=wf, aes(x, y), shape=8, size=4)
  
```

Or, all seasons:

```{r, fig.width=9, fig.height=7}
ggplot() + geom_tile(data=diffdata, aes(x=x.pos, y=y.pos, fill=mediandiff), 
                     height=0.5, width=0.5) + 
  geom_point(data=filter(diffdata, marker == 1), aes(x=x.pos, y=y.pos), 
             shape=3, colour='darkgrey', size=1) +
  geom_point(data=filter(diffdata, marker == -1), aes(x=x.pos, y=y.pos), 
             shape=1, colour='darkgrey', size=1.5) +
  theme_bw() + coord_equal() + facet_wrap(~season) + 
  scale_fill_distiller(palette = "Spectral",name="Difference") + 
  geom_point(data=wf, aes(x, y), shape=8, size=4)
```

## Other useful functions in the MRSea package

I will use the best model above to run some additional diagnostics.

```{r}
finalmod<-salsa2dOutput$bestModel
```

### `runDiagnostics`

This function creates observed vs fitted and fitted vs scaled pearsons residual plots.

```{r, fig.width=9, fig.height=6}
runDiagnostics(finalmod)
```

### Influence diagnostics

These functions assess the influence of different blocks on the data. 

```{r}
timeInfluenceCheck(finalmod, id = data$blockid)
```

```{r, fig.width=9, fig.height=6}
runInfluence(finalmod, id = data$blockid)
```
### Cumulative residual plots

Cumulative residual plots are returned for residuals ordered by each covariate in `varlist`, predicted value and index of observations (temporally). The blue dots are the residuals and the black line is the line of cumulative residual. On the covariate plots (those in `varlist`) the grey line indicates what we would expect from a well fitted covariate. i.e. one that is fitted with excessive knots.

```{r,fig.width=9, fig.height=7}
plotCumRes(model = finalmod, varlist = 'depth')
```


### Making any `glm` model into an `gamMRSea` object

This can be done using the `make.gamMRSea` function.  

```{r}
fullModel.gamMRSea <- make.gamMRSea(fullModel, panelid = data$blockid, gamMRSea = TRUE)
summary(fullModel.gamMRSea)
```

Additionally, if you choose to fit the salsa models without a panel structure, the `make.gamMRSea` function can be used to add a panel structure afterward. Assuming model m1 is a model output from either `runSALSA1D` or `runSALSA2D`:

```{r, eval=FALSE}
m1.robustse <- make.gamMRSea(m1, panelid =  data$blockid)
```